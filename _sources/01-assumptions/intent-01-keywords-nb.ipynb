{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0c80a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coconut'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4708/3033039193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reload_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coconut'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mreload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mreload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_load_ipython_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/share/miniconda/envs/text-data/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coconut'"
     ]
    }
   ],
   "source": [
    "%reload_ext coconut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e9a5b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link href=\"//cdn.jsdelivr.net/npm/mana-font@latest/css/mana.min.css\" rel=\"stylesheet\" type=\"text/css\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935bd19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Keywords \n",
    "\n",
    "> \"Quality Content\" and Where to Find It..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f16f2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first step toward quantifying _something_ is to ask quantifying questions about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fcaea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> What? How much? How many? How often?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a765bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These questions help form a narrative (what am I interested in?), and _sell_ that narrative (why should I be interested?)\n",
    "\n",
    "In essence, we are looking for good _content_, the \"stuff\" that is useful or interesting from the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5075f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15542d47",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's grab one of our course datasets: MTGJson, as documented [in the appendix](content/appendix/datasets/mtgjson). If you're following along, DVC can grab the data, as well: `dvc import...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa702912",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tlp.data import DataLoader\n",
    "df = DataLoader.mtg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44a3c01",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tlp.data import mtg, styleprops_longtext\n",
    "\n",
    "(df[['name', 'text','flavor_text']]\n",
    " .sample(10, random_state=2).fillna('').style\n",
    " .set_properties(**styleprops_longtext(['text','flavor_text']))\n",
    " .hide_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc89a82",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_Flavor text_ has been a staple of Magic cards for a long time. \n",
    "A lot of players gravitate to it, even more than the game itself. \n",
    "\n",
    "There are easter-eggs, long-running gags, and returning characters. \n",
    "Flavor text is really cool. \n",
    "\n",
    "That sounds like some interesting \"content\"...what is its history?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dab8227",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    },
    "tags": [
     "margin"
    ]
   },
   "source": [
    "{figure-md} feeling-lost\n",
    "![fblthp](https://static.wikia.nocookie.net/mtgsalvation_gamepedia/images/c/c4/Fblthp.jpg)\n",
    "\n",
    "_Magic: The Gathering_ can be a lot to take in, and it's easy to get lost in all the strange words. \n",
    "This is why we use it for TLP! \n",
    "Thankfully, us \"lost\" folks have a mascot, in old Fblthp, here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173902d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "(df\n",
    " .set_index('release_date')\n",
    " .sort_index()\n",
    " .resample('Y')\n",
    " .apply(lambda grp: grp.flavor_text.notna().sum()/grp.shape[0])\n",
    "#  .apply(lambda grp: )\n",
    ").plot( rot=45, title='What fraction of cards have Flavor Text each year?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d72bfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There's a lot of other data avaliable, as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ebb243",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mtg.style_table(df.sample(10, random_state=2),\n",
    "                        hide_columns=['text','flavor_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8f45d",
   "metadata": {
    "cell_style": "split",
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def value_ct_wordcloud(s: pd.Series):\n",
    "    from wordcloud import WordCloud\n",
    "    wc = (WordCloud(background_color=\"white\", max_words=50)\n",
    "          .generate_from_frequencies(s.to_dict()))\n",
    "    plt.figure()\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "(df.types.explode().value_counts()\n",
    " .pipe(value_ct_wordcloud)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c7baa",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What \"types\" of cards are there?\n",
    "What are the \"subtypes\" of cards, and how are they differentiate from \"types\"?\n",
    "\n",
    "\n",
    "It looks like this magic is quite anthropocentric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accd2b4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(df.subtypes.explode().value_counts()\n",
    " .pipe(value_ct_wordcloud) \n",
    ")\n",
    "# df.subtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f06e86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Keywords**\n",
    "\n",
    "These kinds of comma-separated lists of \"content-of interest\" are generally called keywords. \n",
    "Here, we have been _told_ what those keywords are, which is nice!\n",
    "\n",
    "Question... would we always have been able to find them from the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757074f",
   "metadata": {
    "cell_style": "split",
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_textual_occurrence(\n",
    "    df,\n",
    "    key_col='keywords', \n",
    "    txt_col='text',\n",
    "    pre = str # do nothing, make str\n",
    "): \n",
    "\n",
    "    def keyword_in_txt(df_row):\n",
    "        return (\n",
    "            pre(df_row[key_col]) \n",
    "            in \n",
    "            pre(df_row[txt_col])\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        df[['text','keywords']].explode('keywords')\n",
    "        .dropna(subset=['keywords'])\n",
    "        .assign(\n",
    "            textual=lambda df: \n",
    "            df.apply(keyword_in_txt, axis=1)\n",
    "        )\n",
    "        .groupby('keywords').mean()\n",
    "        .sort_values('textual')\n",
    "        .head(40)\n",
    "        .hvplot.barh(\n",
    "            title='Fraction of text containing keyword',\n",
    "            frame_width=250, \n",
    "            frame_height=350\n",
    "        )\n",
    "    )\n",
    "\n",
    "plot_textual_occurrence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcce741",
   "metadata": {
    "cell_style": "split",
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# wait...let's lowercase\n",
    "plot_textual_occurrence(\n",
    "    df, pre=lambda s: str(s).lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678992d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recap\n",
    "\n",
    "- Content in a document can occur in or alongside the text itself. \n",
    "- Keywords are individual markers of useful content, often comma-separated\n",
    "- Often you need to \"tidy up\" keyword lists with `df.explode('my_keyword_column)`\n",
    "- Keywords can be supplied a priori (by experts, etc.) Use them!\n",
    "- Supplied keywords have become divorced from the text... do they match?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c324c3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sanity-check\n",
    "\n",
    "So: \n",
    "- `interesting content` $\\rightarrow$ `frequent content` \n",
    "  \n",
    "  $\\rightarrow$ `frequent keywords`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e63c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_What assumption(s) did we make just then?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8745d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> - `interesting content` $\\rightarrow$ `frequent content` $\\rightarrow$ `frequent key`**`words`**\n",
    "\n",
    "_What are **words**?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701ffc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are _assuming_ that a \"fundamental unit\" of interesting content is a \"word\". \n",
    "Remember, though, that a \"word\" is not a known concept to the computer... all it knows are \"strings\"\n",
    "\n",
    "Individual characters, or even slices of strings (i.e. _substrings_) don't have any specific meaning to us as concepts (directly). \n",
    "This means there is a fundamental disconnect (and, therefore, a need for _translation_) between strings and words, to allow the assumption above to _work_ in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d355a3",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%coconut\n",
    "def substrings(size) = \n",
    "    \"\"\"return a function that splits stuff into 'size'-chunks and prints as list\"\"\"\n",
    "    groupsof$(size)..> map$(''.join) ..> list ..> print\n",
    "\n",
    "my_str = \"The quick brown fox\"\n",
    "my_str |> substrings(3)\n",
    "my_str |> substrings(4)\n",
    "my_str |> substrings(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342e0d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Only some of these would make sense as \"words\", and that's only if we do some post-processing in our minds (e.g. `own` could be a word, but is that the same as `[ ]own`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb950d38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we: \n",
    "\n",
    "- formalize turning strings into the these concept-compatible \"word\" objects? \n",
    "- Apply this to our text, so we know the concepts available to us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919df7a3",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intro to Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec23aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Preferably, we want to replace the `substrings` function with something that looked like: \n",
    "\n",
    "```python\n",
    "substr(my_str)\n",
    ">>> ['The', 'quick', 'brown', 'fox']\n",
    "```\n",
    "\n",
    "In text-processing, we have names for these _units of text_ that communicate a single concept: **tokens**. \n",
    "The process of breaking strings of text into tokens is called _tokenization_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6c2713",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There's actually a few _special words_ we use in text analysis to refer to meaningful parts of our text, so let's go ahead and define them ({cite}`jurafsky-textbook`): \n",
    "\n",
    "**corpus**\n",
    ": the set of all text we are processing\n",
    "  > _e.g. the text from entire MTGJSON dataset is our corpus_\n",
    "\n",
    "**document** \n",
    ": a unit of text forming an \"observation\" that we can e.g. compare to others\n",
    "  > e.g. each card in MTGJSON is a \"document\" containing a couple sections of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66590499",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**token**\n",
    ": a piece of text that stands for a word. \n",
    "  > the flavor-text for `Mardu Hateblade`: has 15 tokens, excluding punctuation:\n",
    "  >\n",
    "  > _\"There may be little honor in my tactics, but there is no honor in losing.\"_ \n",
    "\n",
    "**types**\n",
    ": unique words in the vocabulary\n",
    "  > For the same card above, there are 15 tokens, but only 13 types (`there`x2, `honor` x2, `in` x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff145d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using Pandas' `.str`\n",
    "\n",
    "There are a number of [very helpful tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html) in the pandas `.str` namespace of the `Series` object. We can return to our card example from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d320924",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "card = df[df.name.str.fullmatch('Mardu Hateblade')]\n",
    "flav = card.flavor_text\n",
    "print(f'{card.name.values}:\\n\\t {flav.values[0]}')\n",
    "# df.iloc[51411].flavor_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f04ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "flav.str.upper()  # upper-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950daa28",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "flav.str.len()  # how long is the string?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599ebde",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**verify**: the number of tokens and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7994e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should be able to split by the spaces...\n",
    "print(flav.str.split(' '), '\\n')\n",
    "print(\"no. tokens: \", flav.str.split(' ').explode().size)\n",
    "print(\"no. types: \",len(flav.str.split(' ').explode().unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83ca4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "wait a minute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07537289",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flav.str.split().explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13b100",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This isn't right! \n",
    "\n",
    "We probably want to split on anything that's not \"letters\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c516a4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flav.str.split('[^A-Za-z]').explode().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a64d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Much better!\n",
    "\n",
    "So what is this devilry? This `[^A-Za-z]` is a pattern --- a _regular expression_ --- for \"things that are _not_ alphabetical characters in upper or lower-case\". Powerful, right? We'll cover this in more detail in the next section. \n",
    "\n",
    "In the meantime, let's take a look again at this workflow pattern:\n",
    "> `tokenize` $\\rightarrow$ `explode`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d43744",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tidy Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c4d26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_but first_...\n",
    "\n",
    "### Tidy Data Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abf57e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's review an incredibly powerful idea from the R community: using _tidy data_. \n",
    "\n",
    "Tidy data is a _paradigm_ to frame your tabular data representation in a _consistent_ and _ergonomic_ way that supports rapid manipulation, visualization, and cleaning. Imagine we had this non-text dataset (from Hadley Wickham's paper _Tidy Data_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8e86b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_untidy = pd.DataFrame(index=pd.Index(name='name', data=['John Smith', 'Jane Doe', 'Mary Johnson']), \n",
    "             data={'treatment_a':[np.nan, 16, 3], 'treatment_b': [2,11,1]})\n",
    "df_untidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c9da1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We could also represent it another way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2096dbe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_untidy.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e70bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I'm sure these might be equally likely to see in someone's excel sheet, entering this data. But, say we want to visualize this table? Or start comparing each of the cases? This is going to take a lot of manipulation every time we want a different thing. \n",
    "\n",
    "For data to be _Tidy Data_, we need 3 things: \n",
    "\n",
    "> 1. Each variable forms a column.\n",
    "> 2. Each observation forms a row.\n",
    "> 3. Each type of observational unit forms a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee432f40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_tidy = df_untidy.reset_index().melt(id_vars=['name'])\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9ffc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suddenly things like comparing, plotting, and counting become trivial with simple table operations. \n",
    "\n",
    "> But doesn't this waste table space? It's so much less compact!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636939b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That's excel talking! The \"wasted space\" is incredibly negligible at this scale, compared to the ergonomic benefit of representing your data **long-form**, with one-observation-per-row. Now you get exactly one column for every variable, and one row for every point of data, making your manipulations much cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebff9e",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.catplot(\n",
    "    data=df_tidy, \n",
    "    y='value', \n",
    "    x='name', \n",
    "    hue='variable', # try commenting/changing to 'col'!\n",
    "    kind='bar'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aec2e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back to Tidy _Text_\n",
    "\n",
    "So, hang on, aren't _documents_ our observational-level? Wouldn't that make e.g. the MTGJSON dataset _already \"tidy\"_??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ae708",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes! \n",
    "\n",
    "But only if we are observing _cards_, which, for things like release date or mana cost, maybe that's true. \n",
    "\n",
    "Instead, we are trying to find (observe) the occurrences of \"interesting content\", which we broke down into _tokens_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d8126",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> _We thus define the tidy text format as being a table with one-token-per-row._\n",
    "> _A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens._\n",
    "> _This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a document-term matrix._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4186c86",
   "metadata": {
    "cell_style": "split",
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%coconut\n",
    "import nltk\n",
    "import janitor as pj\n",
    "nltk.download('punkt')\n",
    "\n",
    "tidy_df = (\n",
    "    df\n",
    "    .add_column('word', wordlists)\n",
    "    .also(df -> print(df.word.head(10)))\n",
    "    .explode('word')\n",
    "    .rename_axis('card_id')\n",
    "    .reset_index()\n",
    ") where: \n",
    "    wordlists = (\n",
    "        df.flavor_text\n",
    "        .fillna('')\n",
    "        .str.lower()\n",
    "        .apply(nltk.tokenize.word_tokenize)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab7ec4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tidy_df.word.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29874b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assumption Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad82e4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Words? Stopwords.\n",
    "\n",
    "> The \"anti-keyword\"\n",
    "\n",
    "Stuff that we say, _a priori_ is uninteresting. Usually articles, pasive being verbs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fdef2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = pd.Series(name='word', data=nltk.corpus.stopwords.words('english'))\n",
    "print(stopwords.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676c0b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**NB**\n",
    "\n",
    "Discussion: stopwords are _very_ context-sensitive decisions. \n",
    "\n",
    "- Can you think of times when these are _not_ good stop words? \n",
    "\n",
    "- When would these terms actually imply interesting \"content\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710be42",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(tidy_df\n",
    " .filter_column_isin(\n",
    "     'word',\n",
    "     nltk.corpus.stopwords.words('english'), \n",
    "     complement=True # so, NOT IN\n",
    " )\n",
    " .word.value_counts().head(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7f644",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This seems to have worked _ok_. \n",
    "\n",
    "Now we can see some interesting \"content\" in terms like \"life\", \"death\", \"world\", \"time\", \"power\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacda04f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> What might we learn from these keywords? What else could we do to investigate them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6bc77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Importance $\\approx$ Frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbc7f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%coconut \n",
    "keywords = (\n",
    "    tidy_df\n",
    "    .assign(**{\n",
    "        'year': df -> df.release_date.dt.year,\n",
    "        'yearly_cnts': df -> df.groupby(['year', 'word']).word.transform('count'),\n",
    "        'yearly_frac': df -> df.groupby('year').yearly_cnts.transform(grp->grp/grp.count().sum())\n",
    "    })\n",
    "    .filter_column_isin(\n",
    "        'word', \n",
    "        ['life', 'death']\n",
    "#         ['fire', 'water']\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb82c75",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=keywords, x='year', y='yearly_cnts',hue='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1882456",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=keywords, x='year', y='yearly_frac',hue='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77db14a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Lessons**: \n",
    "\n",
    "- Frequency can have _many_ causes, few of which correlate to underlying \"importance\"\n",
    "- Starting to measure importance? _relative_ comparisons, ranked. \n",
    "\n",
    "This get's us part of the way toward information-theoretic measures, and other common weighting schemes. More to come in the [Measure & Evaluate]() chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef46e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside: how many keywords in my corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbce71",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **token**\n",
    "> : a piece of text that stands for a word. \n",
    "\n",
    "> **types**\n",
    "> : unique words in the vocabulary\n",
    "\n",
    "So:\n",
    "- num. types is the size of the vocabulary $\\|V\\|$\n",
    "- num. tokens is the size of the corpus $\\|N\\|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242cbfc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Heap's Law: \n",
    "$$ \\|V\\| = k\\|N\\|^\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy_df.groupby(['card_id']).word.agg(['cumcount', lambda s: (~s.duplicated()).cumsum()])\n",
    "# (~tidy_df.word.duplicated()).cumsum()\n",
    "tidy_df.groupby(['card_id']).word.size().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64fa2d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rand = np.random.default_rng()\n",
    "def sample_docs(df, id_col='card_id', shuffles=5, rng=np.random.default_rng()):\n",
    "    samps = []\n",
    "    for i in range(shuffles):\n",
    "        shuff = df.shuffle()\n",
    "        samps+=[pd.DataFrame({\n",
    "            'N': shuff.groupby(id_col).word.size().cumsum(), \n",
    "            'V': (~shuff.word.duplicated()).cumsum()\n",
    "        })]\n",
    "    return pd.concat(samps).reset_index(drop=True).dropna().query('N>=100')\n",
    "\n",
    "heaps = sample_docs(tidy_df)\n",
    "heaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a54acd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def heaps_law(n, k, beta): \n",
    "    return k*n**beta\n",
    "\n",
    "def fit_heaps(data, linearize=False):\n",
    "    if not linearize:\n",
    "        params, _ = curve_fit(\n",
    "            heaps_law,\n",
    "            data.N.values, \n",
    "            data.V.values\n",
    "        )\n",
    "    else: \n",
    "        log_data = np.log(data)\n",
    "        params, _ = curve_fit(\n",
    "            lambda log_n, k, beta: np.log(k) + beta*log_n,\n",
    "            log_data.N.values,\n",
    "            log_data.V.values\n",
    "        )\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d47d2",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heaps_law(heaps, log_scale=False, linearize=False):\n",
    "    params = fit_heaps(heaps, linearize=linearize)\n",
    "    print(f'fit: k={params[0]:.2f}\\tβ={params[1]:.2f}\\tlinear-fit={linearize}')\n",
    "    \n",
    "    x = np.linspace(100,6e5)\n",
    "    plt.scatter(heaps.N, heaps.V, )\n",
    "    plt.plot(\n",
    "        x, \n",
    "        heaps_law(x, params[0], params[1]), \n",
    "        color='orange', lw=3, label=f'Heaps\\' (β={params[1]:.2f})'\n",
    "    )\n",
    "    plt.fill_between(x, \n",
    "                     heaps_law(x, params[0], 0.67),\n",
    "                     heaps_law(x, params[0], 0.75),\n",
    "                    color='grey', alpha=.2, \n",
    "                    label='typical-range')\n",
    "\n",
    "    plt.ylim(1,heaps.V.max()+1000)\n",
    "    plt.plot(x, np.sqrt(x), ls='--', label='sqrt', color='k')\n",
    "    if log_scale:\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "    plt.legend()\n",
    "plot_heaps_law(heaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f683697",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, our data grows in _complexity_ a lot faster than the square-root of it's size, but slower than \"typical\" text. \n",
    "\n",
    "Most data-sets in NLP are between 0.67-0.75, so we \n",
    "- get a lot of complexity early on, but ...\n",
    "- there's not such an extended amout of \"new concepts\" to find, after a while. \n",
    "\n",
    "> Pretty typical of \"technical\", or, _synthetic_ and domain-centric language. Lot's of variety _initially_, but limited in scope compared to casual speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dd653",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_heaps_law(heaps, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081985cd",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_heaps_law(heaps, log_scale=True, \n",
    "               linearize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5606c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heaps_law(heaps, log_scale=False, linearize=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "source_map": [
   15,
   24,
   34,
   40,
   44,
   48,
   54,
   58,
   62,
   71,
   86,
   96,
   105,
   122,
   126,
   135,
   158,
   166,
   178,
   187,
   228,
   241,
   251,
   260,
   264,
   270,
   278,
   295,
   299,
   306,
   310,
   322,
   334,
   346,
   352,
   363,
   371,
   379,
   383,
   394,
   398,
   406,
   412,
   420,
   429,
   433,
   439,
   445,
   455,
   459,
   467,
   477,
   486,
   492,
   496,
   512,
   518,
   526,
   532,
   560,
   569,
   573,
   581,
   591,
   601,
   617,
   623,
   627,
   631,
   652,
   660,
   668,
   677,
   681,
   693,
   698,
   704,
   725,
   752,
   784,
   794,
   803,
   813
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}